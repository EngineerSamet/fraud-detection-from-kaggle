# âš¡ Credit Card Fraud Detection - Quick Start Guide

![Python](https://img.shields.io/badge/Python-3.11%2B-blue?logo=python)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3%2B-orange?logo=scikit-learn)
![LightGBM](https://img.shields.io/badge/LightGBM-4.0%2B-green?logo=microsoft)
![Runtime](https://img.shields.io/badge/Runtime-~12--15%20min-yellow)

**ğŸ¯ Get the champion fraud detection model running in 5 minutes!**

---

## ğŸ“‹ Prerequisites

- **Python 3.11+** (recommended) or Python 3.8+
- **8GB RAM** minimum (16GB recommended)
- **500MB** disk space for dataset + outputs
- **Internet connection** for dataset download

---

## ğŸš€ Installation (3 Steps)

### Step 1: Clone/Download Project

```bash
git clone https://github.com/EngineerSamet/fraud-detection.git
cd fraud-detection
```

Or download ZIP and extract.

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

**Dependencies installed:**
- `numpy`, `pandas`, `scipy` - Core data science libraries
- `scikit-learn` - Base ML framework
- `xgboost`, `lightgbm` - Gradient boosting models
- `imbalanced-learn` - Imbalance handling (EasyEnsemble, BalancedBagging)
- `matplotlib`, `seaborn` - Visualization
- `shap` - Model interpretability
- `streamlit`, `plotly` - Web application & interactive charts
- `tqdm` - Progress bars

**Installation time:** ~2-3 minutes

### Step 3: Download Dataset âš ï¸ **REQUIRED BEFORE TRAINING**

1. Visit: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
2. Click **Download** (143 MB)
3. Extract `creditcard.csv`
4. Place in: `data/raw/creditcard.csv`

**Folder structure:**
```
ML Project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ creditcard.csv  â† PUT FILE HERE (MANDATORY)
â”œâ”€â”€ main.py
â”œâ”€â”€ predict_fraud.py
â””â”€â”€ requirements.txt
```

**âš ï¸ IMPORTANT:** You MUST download the dataset before running `python main.py`. The training pipeline will fail if `creditcard.csv` is missing.

---

## ğŸƒ Run Training Pipeline

### Execute Full Pipeline

```bash
python main.py
```

**What happens:**

```
[Phase 1] Data Exploration & Preprocessing      (1-2 min)
  âœ… Load 284,807 transactions
  âœ… Detect class imbalance (0.17% fraud)
  âœ… Outlier removal with IQR method
  âœ… Anomaly detection with IsolationForest

[Phase 2] Imbalance Strategy Comparison         (2-3 min)
  âœ… Test class weights, EasyEnsemble, BalancedBagging
  âœ… Train baseline models

[Phase 3] Model Training (17 Configurations)    (4-6 min)
  âœ… Logistic Regression (baseline + class weights)
  âœ… Random Forest (baseline, class weights, optimized)
  âœ… XGBoost (baseline, scale_pos_weight, optimized)
  âœ… LightGBM (class weights, optimized F2) â­ CHAMPION
  âœ… Voting Ensemble (LGBM + XGB + RF)
  âœ… EasyEnsemble Classifier
  âœ… BalancedBagging Classifier

[Phase 4] Model Calibration                      (1 min)
  âœ… Isotonic calibration for LGBM, XGB, RF
  âœ… Brier score analysis

[Phase 5] Threshold Optimization                 (2 min)
  âœ… F2-Score optimization (LGBM)
  âœ… Cost-sensitive analysis (4 ratios for XGB & LGBM)
  âœ… Youden's J statistic

[Phase 6] SHAP Analysis                          (2-3 min)
  âœ… Global feature importance (XGBoost)
  âœ… Fraud-only SHAP analysis
  âœ… Feature interaction detection

[Phase 7] Cross-Validation                       (2 min)
  âœ… 5-fold stratified CV (4 models)
  âœ… Stability analysis

[Phase 8] Results Saving                         (10 sec)
  âœ… Save models to outputs/fraud_detection_final/models/
  âœ… Save results to outputs/fraud_detection_final/results/
  âœ… Generate 48 visualizations (41 main figures + 7 SHAP plots)
```

**Total Runtime:** ~12-15 minutes on modern CPU (Intel i5/i7, AMD Ryzen 5/7)

---

## ğŸ What You'll Get

### ğŸ“Š Trained Models (9 files)

**Location:** `outputs/fraud_detection_final/models/`

1. **`lightgbm_champion.pkl`** - Champion model (88.07% PR-AUC)
2. **`xgboost.pkl`** - XGBoost baseline model
3. **`xgboost_calibrated_isotonic.pkl`** - XGBoost calibrated (87.88% PR-AUC)
4. **`random_forest.pkl`** - Random Forest baseline
5. **`random_forest_calibrated.pkl`** - Random Forest calibrated (84.95% PR-AUC)
6. **`logistic_regression.pkl`** - Logistic Regression baseline (72.13% PR-AUC)
7. **`voting_ensemble.pkl`** - Voting ensemble LGBM+XGB+RF (87.24% PR-AUC)
8. **`balanced_bagging_hybrid.pkl`** - BalancedBagging (58.46% PR-AUC)
9. **`isolation_forest_feature.pkl`** - Anomaly detection model

---

### ğŸ“ˆ Results Files (8 files)

**Location:** `outputs/fraud_detection_final/results/`

1. **`model_comparison.csv`** - 17 models ranked by PR-AUC
2. **`lgbm_cost_sensitivity.json`** - LGBM cost-sensitive thresholds (4 ratios)
3. **`cost_sensitivity_analysis.json`** - XGBoost cost analysis (4 ratios)
4. **`optimal_thresholds.json`** - F2/Youden's J thresholds
5. **`cross_validation_results.json`** - 5-fold CV stability (4 models)
6. **`fraud_only_shap_features.csv`** - Fraud-specific feature importance
7. **`top_shap_features.csv`** - Global SHAP rankings
8. **`all_results.json`** - Complete metrics for all models

---

### ğŸ–¼ï¸ Visualizations (38+ PNG files)

**Location:** `outputs/fraud_detection_final/figures/`

#### Class Imbalance & Data Exploration
- `class_distribution.png` - 99.83% normal vs 0.17% fraud
- `fraud_vs_normal_distributions.png` - Feature separation analysis
- `correlation_heatmap.png` - Feature correlations
- `pca_explained_variance.png` - PCA component importance
- `tsne_fraud_vs_normal.png` - 2D clustering visualization

#### Model Performance (17 models Ã— 3 plots each)
- Confusion matrices for all configurations
- PR Curves (precision-recall trade-off)
- ROC Curves (TPR vs FPR)
- `model_comparison_bar.png` - PR-AUC comparison chart

#### Threshold Optimization
- `threshold_analysis_LGBM.png` - Cost-sensitive threshold optimization
- `LGBM_optimal_f2_confusion_matrix.png` - F2-optimized results
- `calibration_comparison_LGBM.png` - Calibration curve analysis

#### SHAP Interpretability (7 files in `shap/` folder)
- `fraud_only_shap_summary.png` â­ **Fraud-specific feature importance**
- `fraud_only_shap_bar.png` - Mean |SHAP| for fraud cases
- `XGBoost (with Anomaly Score)_shap_summary.png` - Global SHAP
- `XGBoost (with Anomaly Score)_shap_bar.png` - Feature rankings
- `shap_interaction.png` - Feature interactions

#### Anomaly Detection
- `isolationforest_anomaly_scores.png` - Fraud vs Normal anomaly scores

---

## ğŸ§ª Test Deployment Pipeline

### Option 1: Streamlit Web Application (Interactive Demo) ğŸŒ **RECOMMENDED**

Launch the interactive web interface for real-time fraud detection:

```bash
streamlit run app.py
```

**What You'll Get:**
- ğŸ¨ **Beautiful Web Interface** - Professional dashboard with charts and visualizations
- ğŸ“Š **Real Transaction Examples** - Test with 18 pre-loaded examples (9 fraud + 9 normal)
- ğŸ›ï¸ **Manual Input** - Enter custom transaction data
- ğŸ“ˆ **Live Predictions** - See probability gauges, confidence levels, and recommendations
- ğŸ” **Threshold Comparison** - Compare all 6 threshold strategies side-by-side
- ğŸ“‰ **Feature Analysis** - View top SHAP features and anomaly scores

**Access:** Browser will open automatically at `http://localhost:8501`

**Demo Features:**
- âœ… Load real examples with one click
- âœ… Adjust threshold strategies interactively
- âœ… See fraud probability gauge (0-100%)
- âœ… View detailed SHAP feature importance
- âœ… Export predictions to CSV

**Perfect for:** Presentations, demonstrations, and interactive testing

---

### Option 2: Command-Line Testing (Quick Test)

Test the production inference pipeline via terminal:

```bash
python predict_fraud.py
```

**Output Example:**

```
=============================================================================
               ğŸ”’ FRAUD DETECTION PREDICTION SYSTEM ğŸ”’
=============================================================================

ğŸ“¦ Loading models...
   âœ… LightGBM model loaded
   âœ… IsolationForest loaded

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

                           ğŸ” PREDICTION RESULTS ğŸ”

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸš¨ Fraud Probability: 99.99%
ğŸ¯ Risk Level: CRITICAL
ğŸ›¡ï¸  Recommended Action: BLOCK IMMEDIATELY

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Threshold Analysis                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Strategy           â”‚ Threshold   â”‚ Prediction â”‚ Confidence            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ default            â”‚ 0.50        â”‚ FRAUD      â”‚ ğŸ”´ Critical          â”‚
â”‚ f2_optimized       â”‚ 0.60        â”‚ FRAUD      â”‚ ğŸ”´ Critical          â”‚
â”‚ cost_50            â”‚ 0.30        â”‚ FRAUD      â”‚ ğŸ”´ Critical          â”‚
â”‚ cost_100           â”‚ 0.23        â”‚ FRAUD      â”‚ ğŸ”´ Critical          â”‚
â”‚ cost_200           â”‚ 0.23        â”‚ FRAUD      â”‚ ğŸ”´ Critical          â”‚
â”‚ cost_500           â”‚ 0.23        â”‚ FRAUD      â”‚ ğŸ”´ Critical          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Performance Summary

### Champion Model: LightGBM with F2-Optimized Threshold

| Metric          | Value   | Interpretation                                    |
|-----------------|---------|---------------------------------------------------|
| **PR-AUC**      | 88.07%  | Excellent fraud detection (gold standard metric)  |
| **Recall**      | 83.87%  | Catches 78 out of 93 frauds in test set          |
| **Precision**   | 96.30%  | 96 out of 100 alerts are real frauds              |
| **F2-Score**    | 86.09%  | Balanced metric (recall-weighted)                 |
| **ROC-AUC**     | 98.88%  | High true positive rate                           |
| **MCC**         | 0.899   | Excellent correlation (robust metric)             |

**Why This is the Champion:**
- Same base model as LGBM_Calibrated_Isotonic (88.07% PR-AUC)
- **F2-optimized threshold (0.60)** achieves **5.6% higher precision** (96.30% vs 90.70%)
- **62.5% fewer false positives** (3 vs 8 per 56,956 transactions)
- Same PR-AUC and recall â†’ More practical for production deployment
- **Only 3 false alarms** out of 56,863 legitimate transactions

### Top 5 Models by PR-AUC

| Rank | Model Configuration       | PR-AUC | Recall | Precision | False Positives |
|------|---------------------------|--------|--------|-----------|----------------:|
| 1    | LGBM_Optimized_F2 ğŸ†     | 88.07% | 83.87% | 96.30%    | 3               |
| 2    | LGBM_Calibrated_Isotonic  | 88.07% | 83.87% | 90.70%    | 8               |
| 3    | LGBM_ClassWeights         | 87.96% | 84.95% | 84.04%    | ~16             |
| 4    | XGB_Calibrated_Sigmoid    | 87.91% | 82.80% | 81.91%    | ~18             |
| 5    | XGB_Calibrated_Isotonic   | 87.88% | 79.57% | 90.24%    | ~8              |

**Key Insight:** LGBM_Optimized_F2 has same PR-AUC and recall as #2 but **62.5% fewer false positives**

**LGBM Champion - Business Scenario (FN costs more than FP):**

| FN/FP Ratio | Threshold | Recall | Precision | Cost Reduction | Use Case               |
|-------------|-----------|--------|-----------|----------------|------------------------|
| 50:1        | 0.23      | 86.02% | 85.11%    | 12.40%         | Moderate risk          |
| **100:1** â­ | **0.23**  | **86.02%** | **85.11%** | **12.86%** | **Banking standard**   |
| 200:1       | 0.23      | 85.11% | 85.11%    | 13.10%         | High-risk industry     |
| 500:1       | 0.23      | 86.02% | 85.11%    | 13.24%         | Critical infrastructure|

**Real-World Performance (Test Set: 56,956 transactions, 93 frauds):**
- **Default threshold (0.50):** 78 caught, 15 missed, 8 false alarms â†’ Cost: $1,508
- **F2-Optimized (0.60):** 78 caught, 15 missed, **3 false alarms** â†’ Cost: $1,503 (62.5% fewer FPs)
- **Cost-Sensitive (0.23):** 80 caught, 13 missed, 14 false alarms â†’ Cost: $1,314 (12.86% savings)

**Recommendation:** 
- **Production Default:** F2-Optimized (0.60) - Best precision (96.30%) with strong recall
- **High-Risk Periods:** Cost-Sensitive (0.23) - Catches 2 more frauds but 11 more false alarms

---

## ğŸ”§ Troubleshooting

### Issue: "FileNotFoundError: creditcard.csv not found"

**Solution:**
```bash
# Check if file exists
Test-Path "data\raw\creditcard.csv"

# If False, download from:
# https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
```

### Issue: "ImportError: No module named 'lightgbm'"

**Solution:**
```bash
pip install lightgbm>=4.0.0
```

### Issue: "MemoryError during training"

**Solution:** Reduce dataset size for testing:
```python
# In main.py, after loading data (line ~120):
df = df.sample(n=50000, random_state=42)  # Use 50k transactions
```

### Issue: "SHAP analysis takes too long"

**Solution:** Reduce SHAP sample size:
```python
# In src/shap_analysis.py, line ~53:
X_sample = X_data.sample(n=500, random_state=42)  # Use 500 samples
```

---

## ğŸ¯ Next Steps

### 1. Launch Interactive Demo (5 min) ğŸŒŸ **START HERE**
```bash
streamlit run app.py
```
- Best way to understand the project
- Interactive fraud detection interface
- Test real examples with one click
- Perfect for presentations

### 2. Explore Results (5 min)
- Open `outputs/fraud_detection_final/results/model_comparison.csv`
- View top 5 models and their metrics
- Check cost-sensitive analysis results

### 3. View Visualizations (10 min)
- Confusion matrices show prediction accuracy
- PR curves compare model performance
- SHAP plots explain feature importance

### 4. Test Command-Line Pipeline (Optional)
```bash
python predict_fraud.py
```
- Terminal-based fraud prediction demo
- Shows all 6 threshold strategies
- Good for automation/scripting

### 5. Read Full Documentation (30 min)
- See `README.md` for complete methodology
- Understand why class weights beat SMOTE
- Learn about model selection rationale

---

## ğŸ“š Key Learnings

### Why LightGBM Won?

**LGBM_Optimized_F2 (Champion) compared to alternatives:**

**vs LGBM_Calibrated_Isotonic (Base Model):**
- âœ… Same PR-AUC (88.07%)
- âœ… Same recall (83.87%)
- âœ… **5.6% higher precision** (96.30% vs 90.70%)
- âœ… **62.5% fewer false alarms** (3 vs 8 per 56,956 transactions)
- âœ… Optimized threshold (0.60) balances precision and recall perfectly

**Compared to XGBoost:**
- âœ… Better PR-AUC (88.07% vs 87.91%)
- âœ… Higher precision (96.30% vs 90.24% for XGB_Calibrated_Isotonic)
- âœ… Faster training (gradient-based tree growth)
- âœ… Better calibration (Brier score: 0.034 vs 0.041)

**Compared to Random Forest:**
- âœ… Higher PR-AUC (88.07% vs 84.95%)
- âœ… Better recall (83.87% vs 78.49%)
- âœ… More efficient (boosting vs bagging)
- âœ… Faster inference time

**Compared to Ensembles:**
- âœ… Simpler deployment (single model vs 3 models)
- âœ… Similar performance (88.07% vs 87.24% for Voting ensemble)
- âœ… Lower latency in production

### Why Class Weights > SMOTE?

**SMOTE was tested and REJECTED:**
- âŒ Creates synthetic samples in PCA space
- âŒ Risk of unrealistic feature combinations
- âŒ Lower performance in our tests
- âœ… Class weights adjust loss function naturally
- âœ… No fake data generation
- âœ… Better results (88% vs estimated 78% with SMOTE)

**Evidence:** Check `src/imbalance.py` header for detailed explanation

### What is PR-AUC?

**PR-AUC (Precision-Recall Area Under Curve)** is the best metric for imbalanced data.

- **Why not Accuracy?** â†’ 99.8% by predicting all "normal" (useless!)
- **Why not ROC-AUC?** â†’ Can be misleading with 0.17% fraud rate
- **PR-AUC focuses on minority class** â†’ Perfect for fraud detection

**Interpretation:**
- 50% = Random guessing
- 70% = Good
- 80% = Very good
- **88% = Excellent** â­

---

## ğŸ’¬ Support

**Questions?** Contact:
- **Email:** sametsanlikan@gmail.com
- **GitHub:** [@EngineerSamet](https://github.com/EngineerSamet)

---

## â­ Project Highlights

âœ… **Production-Ready:** Deployment pipeline included (`predict_fraud.py`)  
âœ… **Academic Rigor:** 17 model configurations tested  
âœ… **Interpretable:** SHAP analysis explains predictions  
âœ… **Business-Aware:** Cost-sensitive thresholds for real scenarios  
âœ… **Well-Documented:** Comprehensive README + Quick Start  
âœ… **Reproducible:** Fixed random seeds, exact dependency versions  

---

**ğŸ‰ Happy Fraud Detection!**

If this guide helped you, please â­ star the project!
